version: '2.3'
services:
  kafka:
    image: confluentinc/cp-kafka:4.0.3
    ports:
      - 9093:9093
    depends_on:
      zookeeper:
        condition: service_healthy
    links:
      - zookeeper
    environment:
      - "KAFKA_ADVERTISED_LISTENERS=INSIDE://kafka:9092,OUTSIDE://localhost:9093"
      - "KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=INSIDE:PLAINTEXT,OUTSIDE:PLAINTEXT"
      - "KAFKA_LISTENERS=INSIDE://0.0.0.0:9092,OUTSIDE://0.0.0.0:9093"
      - "KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181"
      - "KAFKA_INTER_BROKER_LISTENER_NAME=INSIDE"
      - "KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1"
    restart: on-failure
    healthcheck:
      test: nc -z localhost 9092
      start_period: 10s
      interval: 10s
      timeout: 10s
      retries: 5
  kafka_cli:
    image: confluentinc/cp-kafka:4.0.3
    tty: true
    volumes:
      - ..:/app
    working_dir: /app
    command: bash
    depends_on:
      zookeeper:
        condition: service_healthy
      kafka:
        condition: service_healthy
  kafka_topics_init:
    image: confluentinc/cp-kafka:4.0.3
    tty: true
    volumes:
      - ..:/app
    working_dir: /app
    command: bash -c /app/scripts/create_topics.sh
    depends_on:
      zookeeper:
        condition: service_healthy
      kafka:
        condition: service_healthy
  zookeeper:
    image: confluentinc/cp-zookeeper:4.0.3
    environment:
      - ZOOKEEPER_CLIENT_PORT=2181
    ports:
      - 2181:2181
    restart: on-failure
    healthcheck:
      test: nc -z localhost 2181
      start_period: 10s
      interval: 10s
      timeout: 10s
      retries: 10
  sparkmaster:
    image: gettyimages/spark:2.3.0-hadoop-2.8
    command: bin/spark-class org.apache.spark.deploy.master.Master -h sparkmaster
    environment:
      MASTER: spark://sparkmaster:7077
      SPARK_CONF_DIR: /app/conf/spark
      SPARK_PUBLIC_DNS: localhost
      SPARK_KAFKA_VERSION: 0.10
    ports:
      - 6066:6066
      - 7077:7077
      - 8080:8080
  sparkworker:
    image: gettyimages/spark:2.3.0-hadoop-2.8
    command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://sparkmaster:7077
    hostname: sparkworker
    environment:
      SPARK_CONF_DIR: /app/conf/spark
      SPARK_WORKER_CORES: 2
      SPARK_WORKER_MEMORY: 1g
      SPARK_WORKER_PORT: 8881
      SPARK_WORKER_WEBUI_PORT: 8081
      SPARK_PUBLIC_DNS: localhost
      SPARK_KAFKA_VERSION: 0.10
    depends_on:
      - sparkmaster
    ports:
      - 8081:8081
      - 8881:8881
  spark_cli:
    image: gettyimages/spark:2.3.0-hadoop-2.8
    command: bash
    volumes:
      - ..:/app
    working_dir: /app
    tty: true
    environment:
      SPARK_KAFKA_VERSION: 0.10
    ports:
      - 4040:4040
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.4.2
    ports:
      - 9200:9200
      - 9300:9300
    environment:
      - "discovery.type=single-node"
volumes:
  esdata:
    driver: local
